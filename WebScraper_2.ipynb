{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.indeed.com/jobs?q=software+engineer+%2420%2C000&l=New+York&start=10\"\n",
    "#conducting a request of the stated URL above:\n",
    "page = requests.get(URL)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "              jobs.append(a[\"title\"])\n",
    "    return(jobs)\n",
    "extract_job_title_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_result(soup): \n",
    "    companies = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return(companies)\n",
    " \n",
    "extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location_from_result(soup): \n",
    "    locations = []\n",
    "    spans = soup.findAll(\"span\", attrs={\"class\": \"location\"})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    return(locations)\n",
    "extract_location_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_from_result(soup): \n",
    "    salaries = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        try:\n",
    "            salaries.append(div.find(\"nobr\").text)\n",
    "        except:\n",
    "            try:\n",
    "                div_two = div.find(name=\"div\", attrs={\"class\":\"sjcl\"})\n",
    "                div_three = div_two.find(\"div\")\n",
    "                salaries.append(div_three.text.strip())\n",
    "            except:\n",
    "                salaries.append(\"Nothing_found\")\n",
    "    return(salaries)\n",
    "extract_salary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_result(soup): \n",
    "    summaries = []\n",
    "    spans = soup.findAll(\"span\", attrs={\"class\": \"summary\"})\n",
    "    for span in spans:\n",
    "        summaries.append(span.text.strip())\n",
    "    return(summaries)\n",
    "extract_summary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_results_per_city = 100\n",
    "city_set = [\"New+York\",\"Chicago\",\"San+Francisco\", \"Austin\", \"Seattle\", \"Los+Angeles\", \"Philadelphia\", \"Atlanta\", \"Dallas\", \"Pittsburgh\", \"Portland\", \"Phoenix\", \"Denver\", \"Houston\", \"Miami\", \"Washington+DC\", \"Boulder\"]\n",
    "columns = [\"city\", \"job_title\", \"company_name\", \"location\", \"summary\", \"salary\"]\n",
    "sample_df = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in city_set:\n",
    "    for start in range(0, max_results_per_city, 20):\n",
    "        page = requests.get(\"http://www.indeed.com/jobs?q=software+engineer+%2420%2C000&l=\" + str(city) + \"&start=\" + str(start))\n",
    "        time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}): \n",
    "            #specifying row num for index of job posting in dataframe\n",
    "            num = (len(sample_df) + 1) \n",
    "            #creating an empty list to hold the data for each posting\n",
    "            job_post = [] \n",
    "            #append city name\n",
    "            job_post.append(city) \n",
    "            #grabbing job title\n",
    "            for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "                job_post.append(a[\"title\"]) \n",
    "            #grabbing company name\n",
    "            company = div.find_all(name=\"span\", attrs={\"class\":\"company\"}) \n",
    "            if len(company) > 0: \n",
    "                for b in company:\n",
    "                    job_post.append(b.text.strip()) \n",
    "            else: \n",
    "                sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "                for span in sec_try:\n",
    "                    job_post.append(span.text) \n",
    "            #grabbing location name\n",
    "            c = div.findAll(\"span\", attrs={\"class\": \"location\"}) \n",
    "            for span in c: \n",
    "                job_post.append(span.text) \n",
    "            #grabbing summary text\n",
    "            d = div.findAll(\"span\", attrs={\"class\": \"summary\"}) \n",
    "            for span in d:\n",
    "                job_post.append(span.text.strip()) \n",
    "            #grabbing salary\n",
    "            try:\n",
    "                job_post.append(div.find(\"nobr\").text) \n",
    "            except:\n",
    "                try:\n",
    "                    div_two = div.find(name=\"div\", attrs={\"class\":\"sjcl\"}) \n",
    "                    div_three = div_two.find(\"div\") \n",
    "                    job_post.append(div_three.text.strip())\n",
    "                except:\n",
    "                    job_post.append(\"Nothing_found\") \n",
    "    #appending list of job post info to dataframe at index num\n",
    "    sample_df.loc[num] = job_post\n",
    "    #saving sample_df as a local csv file — define your own local path to save contents \n",
    "    sample_df.to_csv(\"/Users/Nish/Documents/Acadamic-RIT/Fall 2017/Sample_Dataset.csv\")\n",
    "    \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
